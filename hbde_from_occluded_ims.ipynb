{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwh9gwdna1gK"
   },
   "source": [
    "# Human Body Dimension Estimation (HBDE) from occluded images\n",
    "\n",
    "### Authors: M. Beiwinkler, M. KrimpelstÃ¤tter, I. Viertola and T. Wulz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "IN_HOSTED_COLAB_RUNTIME = 1\n",
    "if IN_HOSTED_COLAB_RUNTIME == 1:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "ABDKL9JwbCAB"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGUlDKWJa1gN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from skimage.io import imread\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, SubsetRandomSampler\n",
    "from torch.nn import Linear, ReLU, MSELoss, Sequential, Conv2d, MaxPool2d, Module, BatchNorm2d, LeakyReLU\n",
    "from torch.optim import SGD\n",
    "\n",
    "import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r25DSxXIa1gP"
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "if IN_HOSTED_COLAB_RUNTIME == 1:\n",
    "    base = \"/content/drive/MyDrive/data\"  # Configure this variable accordingly where your dataset is in Google Drive\n",
    "    TRAIN_TEST_PATH = os.path.join(base, \"train_test_split.json\")\n",
    "else:\n",
    "    base = os.getcwd()\n",
    "    TRAIN_TEST_PATH = os.path.join(base, os.path.join(\"dataset\", os.path.join(\"train_test\", \"train_test_split.json\")))\n",
    "\n",
    "DATASET_PATH = os.path.join(base, \"dataset\")\n",
    "IMS_PATH = os.path.join(DATASET_PATH, os.path.join(\"synthetic_images\", \"200x200\"))\n",
    "ANNOS_PATH = os.path.join(DATASET_PATH, \"annotations\")\n",
    "\n",
    "FULL_DATA_NP_ARRAY_PATH = os.path.join(base, \"im_data.npy\")\n",
    "FULL_DATA_WITH_OCCLUSIONS_NP_ARRAY_PATH = os.path.join(base, \"im_data_with_occlusions.npy\")\n",
    "CHOSEN_ANNOS_NP_ARRAY_PATH = os.path.join(base, \"im_annos.npy\")\n",
    "\n",
    "MODEL_SAVE_PATH = os.path.join(base, \"model.pth\")\n",
    "\n",
    "if not os.path.exists(IMS_PATH):\n",
    "    Exception(\"Dataset does not exist {}\".format(DATASET_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1Z1tsVMa1gR"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbgMewWZa1gR"
   },
   "outputs": [],
   "source": [
    "# Train/test split by img indices\n",
    "f = open(TRAIN_TEST_PATH, \"r\")\n",
    "train_test_split = json.load(f)\n",
    "f.close()\n",
    "\n",
    "# Load data from file or from the dataset\n",
    "if os.path.exists(FULL_DATA_NP_ARRAY_PATH):\n",
    "    im_data = np.load(FULL_DATA_NP_ARRAY_PATH)\n",
    "\n",
    "else:\n",
    "    im_data = []\n",
    "    for fn in sorted(glob.glob(os.path.join(IMS_PATH, os.path.join(\"**\", \"*.png\")), recursive=True)):\n",
    "        img = imread(fn)\n",
    "        img = img.astype('float32')\n",
    "        img /= 255.0\n",
    "        im_data.append(img)\n",
    "\n",
    "    im_data = np.array(im_data)\n",
    "    with open(FULL_DATA_NP_ARRAY_PATH, \"wb+\") as f:\n",
    "        np.save(f, im_data)\n",
    "    f.close()\n",
    "\n",
    "if not im_data.shape == (12000, 200, 200):\n",
    "    Exception(\"Problems with image data.\")\n",
    "\n",
    "# Load annotations. One subject has 2 images (12000 images in total) -> 6000 annotations (1 annotation per subject).\n",
    "if os.path.exists(CHOSEN_ANNOS_NP_ARRAY_PATH):\n",
    "    im_annos = np.load(CHOSEN_ANNOS_NP_ARRAY_PATH)\n",
    "\n",
    "else:\n",
    "    im_annos = []\n",
    "    for anno_fn in sorted(glob.glob(os.path.join(ANNOS_PATH, os.path.join(\"*\", \"*.json\")))):\n",
    "        f = open(anno_fn, \"r\")\n",
    "        annotations = json.load(f)\n",
    "        im_annos.append([annotations['human_dimensions']['height'], annotations['human_dimensions']['shoulder_width'],\n",
    "                            annotations['human_dimensions']['left_arm_length'], annotations['human_dimensions']['right_arm_length'],\n",
    "                            annotations['human_dimensions']['pelvis_circumference'], annotations['human_dimensions']['chest_circumference']])\n",
    "    im_annos = np.array(im_annos)\n",
    "    with open(CHOSEN_ANNOS_NP_ARRAY_PATH, \"wb+\") as f:\n",
    "        np.save(f, im_data)\n",
    "    f.close()\n",
    "\n",
    "if not im_annos.shape == (6000, 6):\n",
    "    Exception(\"Problems with image annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNmrbgnna1gX"
   },
   "source": [
    "# Prepare occluded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tmROko0rqL3"
   },
   "outputs": [],
   "source": [
    "# Load occluded images from file or generate them\n",
    "\n",
    "# Number of occluding rectangles per image. If you change this value, don't forget\n",
    "# to delete any occluded images from disk before running this cell. This way you\n",
    "# force new images with the appropriate number of occluding rectangles to be created.\n",
    "# A value of zero leads to a value of one, two or three being chosen randomly for each image.\n",
    "NUM_RECTANGLES = 1\n",
    "\n",
    "if os.path.exists(FULL_DATA_WITH_OCCLUSIONS_NP_ARRAY_PATH):\n",
    "    im_data_occl = np.load(FULL_DATA_WITH_OCCLUSIONS_NP_ARRAY_PATH)\n",
    "else:\n",
    "    # Seed pseudo-random number generator for reproducibility\n",
    "    random.seed(a=\"HBDE from occluded images\", version=2)\n",
    "\n",
    "    # Add rectangles to images\n",
    "    num_rectangles = NUM_RECTANGLES\n",
    "    im_data_occl = []\n",
    "    for img_index in range(im_data.shape[0]):\n",
    "        # Convert from NumPy array to instance of Pillow Image class\n",
    "        img = Image.fromarray(im_data[img_index])\n",
    "        # Create an object that can be used to draw in the given image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        if NUM_RECTANGLES == 0:\n",
    "            num_rectangles = random.randrange(1, 4)\n",
    "        for rect in range(num_rectangles):\n",
    "            # Determine position, length and width of occluding rectangle\n",
    "            if img_index < 3000: # female in pose 0\n",
    "                upper_left_x = random.randrange(30, 150)\n",
    "                upper_left_y = random.randrange(30, 150)\n",
    "            elif img_index < 6000: # male in pose 0\n",
    "                upper_left_x = random.randrange(15, 170)\n",
    "                upper_left_y = random.randrange(20, 175)\n",
    "            elif img_index < 9000: # female in pose 1\n",
    "                upper_left_x = random.randrange(65, 115)\n",
    "                upper_left_y = random.randrange(30, 150)\n",
    "            else: # male in pose 1\n",
    "                upper_left_x = random.randrange(60, 120)\n",
    "                upper_left_y = random.randrange(20, 175)\n",
    "\n",
    "            lower_right_x = upper_left_x + random.randrange(10, 50)\n",
    "            lower_right_y = upper_left_y + random.randrange(10, 50)\n",
    "\n",
    "            # Draw rectangle\n",
    "            draw.rectangle((upper_left_x, upper_left_y, lower_right_x, lower_right_y), fill=\"black\")\n",
    "\n",
    "        # Add manipulated image to list of occluded images\n",
    "        im_data_occl.append(np.asarray(img))\n",
    "        \n",
    "    im_data_occl = np.array(im_data_occl)\n",
    "    with open(FULL_DATA_WITH_OCCLUSIONS_NP_ARRAY_PATH, \"wb+\") as f:\n",
    "        np.save(f, im_data_occl)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpJbJzZEa1gV"
   },
   "outputs": [],
   "source": [
    "# Divide training and testing data\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_annos = []\n",
    "test_annos = []\n",
    "\n",
    "# FILL TRAIN/TEST DATA WITH OCCLUDED\n",
    "for gender in train_test_split['train']:\n",
    "    for pose in train_test_split['train'][gender]:\n",
    "        for index in train_test_split['train'][gender][pose]:\n",
    "            train_data.append(np.take(im_data_occl, index, axis=0))\n",
    "            index = index - 6000 if index > 5999 else index\n",
    "            train_annos.append(np.take(im_annos, index, axis=0))\n",
    "\n",
    "for gender in train_test_split['test']:\n",
    "    for pose in train_test_split['test'][gender]:\n",
    "        for index in train_test_split['test'][gender][pose]:\n",
    "            test_data.append(np.take(im_data_occl, index, axis=0))\n",
    "            index = index - 6000 if index > 5999 else index\n",
    "            test_annos.append(np.take(im_annos, index, axis=0))\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32)\n",
    "test_data = np.array(test_data, dtype=np.float32)\n",
    "train_annos = np.array(train_annos, dtype=np.float32)\n",
    "test_annos = np.array(test_annos, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_I0xVGRa1gW"
   },
   "outputs": [],
   "source": [
    "# Review data\n",
    "im_data_to_plot = im_data_occl # I hope this won't result in the array to be copied\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(221), plt.imshow(im_data_to_plot[0], cmap='gray'), plt.title(\"Female, pose 0\")\n",
    "plt.subplot(222), plt.imshow(im_data_to_plot[3000], cmap='gray'), plt.title(\"Male, pose 0\")\n",
    "plt.subplot(223), plt.imshow(im_data_to_plot[6000], cmap='gray'), plt.title(\"Female, pose 1\")\n",
    "plt.subplot(224), plt.imshow(im_data_to_plot[9000], cmap='gray'), plt.title(\"Male, pose 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66ifUrhua1gX"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class BodyImages(Dataset):\n",
    "    def __init__(self, X, y, is_training) -> None:\n",
    "        assert X.shape[0] == y.shape[0], \"Sample amounts must match in X and y\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.is_training = is_training\n",
    "\n",
    "        if torch.cuda.is_available(): \n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        super().__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.unsqueeze(torch.tensor(self.X[idx,:,:], requires_grad=True, device=self.device), 0), torch.tensor(self.y[idx, :], device=self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHUKQhP3a1gb"
   },
   "source": [
    "## Create the CNN and define test function\n",
    "Selected features (6): \n",
    "- height\n",
    "- shoulder width\n",
    "- right arm length\n",
    "- left arm length\n",
    "- pelvis circumifrence\n",
    "- chest circumifrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDqPPu3sa1gc"
   },
   "outputs": [],
   "source": [
    "feature_amnt = 6\n",
    "lin_ftr_amnt = 512\n",
    "\n",
    "class CNN(Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Conv2d(in_channels=1, out_channels=feature_amnt, kernel_size=5),\n",
    "            LeakyReLU(inplace=True),\n",
    "            BatchNorm2d(num_features=feature_amnt),\n",
    "            MaxPool2d(kernel_size=5, stride=2),\n",
    "            # Second convolutional layer\n",
    "            Conv2d(in_channels=6, out_channels=(feature_amnt*2), kernel_size=5),\n",
    "            LeakyReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=5, stride=2),\n",
    "        )\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(in_features=23232, out_features=lin_ftr_amnt),\n",
    "            LeakyReLU(inplace=True),\n",
    "            Linear(in_features=lin_ftr_amnt, out_features=feature_amnt)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "\n",
    "def test(dataloader, model, loss_fn, results, fold):\n",
    "    # Enable evaluation mode for model to disable any training-mode specific model behavior,\n",
    "    # just in case our model behaves differently in training and eval mode\n",
    "    # (not to be mistaken for a mechanism to disable gradient computation).\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            results[fold, i:i+len(y), 0] = y.cpu().numpy()\n",
    "            results[fold, i:i+len(pred), 1] = pred.cpu().numpy()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            i += len(y)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmwwcxcEa1gc"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9zCSSY_a1gd"
   },
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "\n",
    "# TODO: Now we only calculate MSE over all the 6 selected features.\n",
    "#       Implement a model validation function where we calculate the MSE over specific feature.\n",
    "#       This way we get a better idea how accurate our model really is for different HBDs.\n",
    "def train(model, optim, loss_fn, dataloader):\n",
    "    # Enable training mode for model to enable possible training-mode specific model behavior,\n",
    "    # just in case our model behaves differently in training and eval mode.\n",
    "    model.train()\n",
    "    current_loss = 0.\n",
    "    with tqdm.tqdm(dataloader, unit='batch') as b:\n",
    "        for batch, (X, y) in enumerate(b):\n",
    "            # Zero-out gradients\n",
    "            optim.zero_grad()\n",
    "            # Forward pass\n",
    "            pred = model(X)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(pred, y)\n",
    "            # Backward pass and gradient computation\n",
    "            loss.backward()\n",
    "            # Parameter update\n",
    "            optim.step()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "lr = 0.001 # lr acc. to. paper: 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "k_folds = 5\n",
    "\n",
    "train_data_ds = BodyImages(train_data, train_annos, is_training=True)\n",
    "test_data_ds = BodyImages(test_data, test_annos, is_training=False)\n",
    "k_fold = KFold(n_splits=k_folds, shuffle=True)\n",
    "dataset = ConcatDataset([train_data_ds, test_data_ds])\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "results = np.empty((5, 2400, 2, 6))\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    device = torch.device('cuda')\n",
    "    # network.cuda()\n",
    "    loss_fn.cuda()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "  \n",
    "print(\"Using:\", device, \"\\n\")\n",
    "\n",
    "# K-Fold cross validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(k_fold.split(dataset)):\n",
    "    print(15 * \"-\" + \"FOLD #{}\".format(fold+1) + 15 * \"-\")\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    testloader = DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "    \n",
    "    network = CNN()\n",
    "    network.apply(reset_weights)\n",
    "    optimizer = SGD(network.parameters(),lr=lr, momentum=momentum)\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "      device = torch.device('cuda')\n",
    "      network.cuda()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(\"-\"*15, \"epoch:\", e + 1, \"-\"*15)\n",
    "        train(network, optimizer, loss_fn, trainloader)\n",
    "\n",
    "    torch.save(network.state_dict(), os.path.join(base, \"model-{}-lr{}-mmnt{}.pth\".format(fold+1, lr, momentum)))\n",
    "    results = test(testloader, network, loss_fn, results, fold)\n",
    "\n",
    "with open(os.path.join(base, \"results-lr{}-mmnt{}.npy\".format(lr, momentum)), \"wb+\") as f:\n",
    "    np.save(f, results)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSDzui40a1hA"
   },
   "source": [
    "# Evaluation\n",
    "k-fold cross validation with 5 folds. Each fold contains 2400 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aE-U7xP8AGBH"
   },
   "outputs": [],
   "source": [
    "#results = np.load('/content/drive/MyDrive/data/results.npy')\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o4xtlPAkpW3"
   },
   "outputs": [],
   "source": [
    "accuracies_mad = []\n",
    "for f in range(results.shape[0]):\n",
    "    accuracies_mad.append(np.mean(np.absolute(results[f, :, 1, :] - results[f, :, 0, :]), axis=0))\n",
    "print(accuracies_mad)\n",
    "accuracies_mad_meanoverk = np.mean(accuracies_mad, axis=0)\n",
    "print(accuracies_mad_meanoverk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L67CcdkdmSbN"
   },
   "outputs": [],
   "source": [
    "accuracies_rpe = []\n",
    "for f in range(results.shape[0]):\n",
    "    accuracies_rpe.append(np.mean(np.absolute(np.divide(results[f, :, 1, :] - results[f, :, 0, :], results[f, :, 0, :])), axis=0))\n",
    "print(accuracies_rpe)\n",
    "accuracies_rpe_meanoverk = np.mean(accuracies_rpe, axis=0)\n",
    "print(accuracies_rpe_meanoverk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxg-BdZHmXD3"
   },
   "outputs": [],
   "source": [
    "print(\"Estimation errors for occluded images:\")\n",
    "print(\"----- MAD\")\n",
    "print(\"  height: {:.3f} mm\".format(accuracies_mad_meanoverk[0] * 1000))\n",
    "print(\"  shoulder_width: {:.3f} mm\".format(accuracies_mad_meanoverk[1] * 1000))\n",
    "print(\"  left_arm_length: {:.3f} mm\".format(accuracies_mad_meanoverk[2] * 1000))\n",
    "print(\"  right_arm_length: {:.3f} mm\".format(accuracies_mad_meanoverk[3] * 1000))\n",
    "print(\"  pelvis_circumference: {:.3f} mm\".format(accuracies_mad_meanoverk[4] * 1000))\n",
    "print(\"  chest_circumference: {:.3f} mm\".format(accuracies_mad_meanoverk[5] * 1000))\n",
    "print()\n",
    "print(\"----- RPE\")\n",
    "print(\"  height: {:.3f}%\".format(accuracies_rpe_meanoverk[0]*100))\n",
    "print(\"  shoulder_width: {:.3f}%\".format(accuracies_rpe_meanoverk[1]*100))\n",
    "print(\"  left_arm_length: {:.3f}%\".format(accuracies_rpe_meanoverk[2]*100))\n",
    "print(\"  right_arm_length: {:.3f}%\".format(accuracies_rpe_meanoverk[3]*100))\n",
    "print(\"  pelvis_circumference: {:.3f}%\".format(accuracies_rpe_meanoverk[4]*100))\n",
    "print(\"  chest_circumference: {:.3f}%\".format(accuracies_rpe_meanoverk[5]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualise results"
   ],
   "metadata": {
    "id": "7u4uM4JKdgEg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SXFH2AG791j"
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/data/model.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Divide training and testing data\n",
    "#train_data_occluded = []\n",
    "test_data_occluded = []\n",
    "\n",
    "#for gender in train_test_split['train']:\n",
    "#    for pose in train_test_split['train'][gender]:\n",
    "#        for indices in train_test_split['train'][gender][pose]:\n",
    "#            train_data_occluded.append(np.take(im_data_occl, indices, axis=0))\n",
    "\n",
    "for gender in train_test_split['test']:\n",
    "    for pose in train_test_split['test'][gender]:\n",
    "        for indices in train_test_split['test'][gender][pose]:\n",
    "            test_data_occluded.append(np.take(im_data_occl, indices, axis=0))\n",
    "\n",
    "#train_data_occluded = np.array(train_data_occluded, dtype=np.float32)\n",
    "test_data_occluded = np.array(test_data_occluded, dtype=np.float32)"
   ],
   "metadata": {
    "id": "-4OH71xrquWT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZH5eJx5AurEV"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "loss_fn = MSELoss()\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn.cuda()\n",
    "    model.cuda()\n",
    "\n",
    "occluded_test_data_ds = BodyImages(test_data_occluded, test_annos, is_training=False)\n",
    "occluded_test_dataloader = DataLoader(occluded_test_data_ds, batch_size=batch_size)\n",
    "occluded_results = test(occluded_test_dataloader, model, loss_fn, np.empty((1, test_annos.shape[0], 2, test_annos.shape[1])), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzmOl42FwZxb"
   },
   "outputs": [],
   "source": [
    "occ_accuracies_mad = []\n",
    "for f in range(occluded_results.shape[0]):\n",
    "    occ_accuracies_mad.append(np.mean(np.absolute(occluded_results[f, :, 1, :] - occluded_results[f, :, 0, :]), axis=0))\n",
    "print(occ_accuracies_mad)\n",
    "occ_accuracies_mad = list(occ_accuracies_mad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvPIiP9Nwag1"
   },
   "outputs": [],
   "source": [
    "occ_accuracies_rpe = []\n",
    "for f in range(occluded_results.shape[0]):\n",
    "    occ_accuracies_rpe.append(np.mean(np.absolute(np.divide(occluded_results[f, :, 1, :] - occluded_results[f, :, 0, :], occluded_results[f, :, 0, :]+.000001)), axis=0))\n",
    "print(occ_accuracies_rpe)\n",
    "occ_accuracies_rpe = list(occ_accuracies_rpe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rAxaV--v_Mn"
   },
   "outputs": [],
   "source": [
    "print(\"Estimation errors for occluded images:\")\n",
    "print(\"----- MAD\")\n",
    "print(\"  height: {:.3f}\".format(occ_accuracies_mad[0]))\n",
    "print(\"  shoulder_width: {:.3f}\".format(occ_accuracies_mad[1]))\n",
    "print(\"  left_arm_length: {:.3f}\".format(occ_accuracies_mad[2]))\n",
    "print(\"  right_arm_length: {:.3f}\".format(occ_accuracies_mad[3]))\n",
    "print(\"  pelvis_circumference: {:.3f}\".format(occ_accuracies_mad[4]))\n",
    "print(\"  chest_circumference: {:.3f}\".format(occ_accuracies_mad[5]))\n",
    "print()\n",
    "print(\"----- RPE\")\n",
    "print(\"  height: {:.3f}%\".format(occ_accuracies_rpe[0]*100))\n",
    "print(\"  shoulder_width: {:.3f}%\".format(occ_accuracies_rpe[1]*100))\n",
    "print(\"  left_arm_length: {:.3f}%\".format(occ_accuracies_rpe[2]*100))\n",
    "print(\"  right_arm_length: {:.3f}%\".format(occ_accuracies_rpe[3]*100))\n",
    "print(\"  pelvis_circumference: {:.3f}%\".format(occ_accuracies_rpe[4]*100))\n",
    "print(\"  chest_circumference: {:.3f}%\".format(occ_accuracies_rpe[5]*100))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot first NUM_ELEMENTS elements of set of occluded images together with abs. differences and rel. perc. errors for each hbd\n",
    "\n",
    "NUM_ELEMENTS = 10\n",
    "\n",
    "abs_diffs = np.absolute(occluded_results[0, 0:NUM_ELEMENTS, 1, :] - occluded_results[0, 0:NUM_ELEMENTS, 0, :])\n",
    "rel_per_errs = np.absolute(np.divide(occluded_results[0, 0:NUM_ELEMENTS, 1, :] - occluded_results[0, 0:NUM_ELEMENTS, 0, :], occluded_results[0, 0:NUM_ELEMENTS, 0, :]+.000001))\n",
    "\n",
    "plt.figure(figsize=(18, 9*(NUM_ELEMENTS+1)/2))\n",
    "for index in range(NUM_ELEMENTS):\n",
    "    # During test items in test dataset should have been queried by used simple\n",
    "    # DataLoader in an ordered manner starting with index zero\n",
    "    #assert test_annos[index, :].all == occluded_results[0, index, 0, :].all\n",
    "    \n",
    "    img = test_data_occluded[index,:,:]\n",
    "    #annos = occluded_results[0, index, 0, :]\n",
    "    #annos = test_annos[index, :]\n",
    "    #predicts = occluded_results[0, index, 1, :]\n",
    "\n",
    "    plt.subplot((NUM_ELEMENTS+1)/2, 2, index+1)\n",
    "    plt.title(f\"{abs_diffs[index]}\\n{rel_per_errs[index]}\")\n",
    "    plt.imshow(img, cmap='gray')\n"
   ],
   "metadata": {
    "id": "xU9Td8wEgBn4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "example_im = np.array([np.take(im_data_occl, 9000, axis=0)])\n",
    "example_ann = np.array([np.take(im_annos, 3000, axis=0)])"
   ],
   "metadata": {
    "id": "pFYrZxPZdjnl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "example_res = np.empty((1, 1, 2, 6))\n",
    "example_dl = DataLoader(BodyImages(example_im, example_ann, is_training=False))\n",
    "example_res = test(example_dl, network, loss_fn, example_res, 0)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(example_im[0], cmap='gray'), plt.title(\"Male pose 1\")\n",
    "\n",
    "mad = np.mean(np.absolute(results[0, :, 1, :] - results[0, :, 0, :]), axis=0)\n",
    "rpe = np.mean(np.absolute(np.divide(results[0, :, 1, :] - results[0, :, 0, :], results[0, :, 0, :])), axis=0)\n",
    "\n",
    "print(\"Estimation errors for example image:\")\n",
    "print(\"----- MAD\")\n",
    "print(\"  height: {:.3f} mm\".format(mad[0] * 1000))\n",
    "print(\"  shoulder_width: {:.3f} mm\".format(mad[1] * 1000))\n",
    "print(\"  left_arm_length: {:.3f} mm\".format(mad[2] * 1000))\n",
    "print(\"  right_arm_length: {:.3f} mm\".format(mad[3] * 1000))\n",
    "print(\"  pelvis_circumference: {:.3f} mm\".format(mad[4] * 1000))\n",
    "print(\"  chest_circumference: {:.3f} mm\".format(mad[5] * 1000))\n",
    "print()\n",
    "print(\"----- RPE\")\n",
    "print(\"  height: {:.3f}%\".format(rpe[0]*100))\n",
    "print(\"  shoulder_width: {:.3f}%\".format(rpe[1]*100))\n",
    "print(\"  left_arm_length: {:.3f}%\".format(rpe[2]*100))\n",
    "print(\"  right_arm_length: {:.3f}%\".format(rpe[3]*100))\n",
    "print(\"  pelvis_circumference: {:.3f}%\".format(rpe[4]*100))\n",
    "print(\"  chest_circumference: {:.3f}%\".format(rpe[5]*100))"
   ],
   "metadata": {
    "id": "vX2RtL51ezK-"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hbde_from_occluded_ims.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "4fdc2f73bf5d480b4cf34a598d9b45a8492d840372cfa5913d0a708f31ebf008"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
  "nbformat_minor": 0
}
